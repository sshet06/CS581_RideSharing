{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from h3 import h3\n",
    "import json\n",
    "from urllib.request import URLError, Request, urlopen\n",
    "from itertools import combinations\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timedelta\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/LaGuardia_Dropoff_Final.csv')\n",
    "columns = ['tpep_pickup_datetime', 'tpep_dropoff_datetime','passenger_count',\\\n",
    "           'trip_distance', 'pickup_longitude','pickup_latitude','dropoff_longitude', 'dropoff_latitude']\n",
    "df = df[columns]\n",
    "df.rename(columns={'tpep_pickup_datetime':'pickup_time',\n",
    "       'tpep_dropoff_datetime':'dropoff_time'},inplace=True)\n",
    "df['pickup_h3'] = df.apply(lambda x: h3.geo_to_h3(x['pickup_latitude'], x['pickup_longitude'], 8), axis=1)\n",
    "df['dropoff_h3'] = df.apply(lambda x: h3.geo_to_h3(x['dropoff_latitude'], x['dropoff_longitude'], 10), axis=1)\n",
    "\n",
    "\n",
    "start_date = '2016-01-29 08:00:00' # Start date with time\n",
    "end_date = '2016-01-30 23:59:59' # End date with time\n",
    "df=df[(df['pickup_time'] >= start_date) & (df['dropoff_time'] <= end_date)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation steps\n",
    "\n",
    "\n",
    "### 1.Install the latest JRE and get GraphHopper Server as zip from <a href=https://graphhopper.com/public/releases/graphhopper-web-0.10.3-bin.zip>Graphhopper API</a>. Unzip it.\n",
    "### 2.Copy this OSM file into the SAME unzipped directory: <a href=https://download.geofabrik.de/north-america/us/new-york-latest.osm.pbf >new-york-latest.osm.pbf</a>\n",
    "### 3.Start GraphHopper Maps via: ava -jar graphhopper-web-0.10.3-with-dep.jar jetty.resourcebase=webapp config=config-example.properties datareader.file=new-york-latest.osm.pbf. \n",
    "### 3.Test to see if its running after you see 'Started server at HTTP 8989' by going to http://localhost:8989/ and you should see a map of New York.\n",
    "### 4.Keep this running when executing our program because this is the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance = 14964.969\n",
      "Time = 892052\n"
     ]
    }
   ],
   "source": [
    "#Check how graphhopper works\n",
    "a,b,c,d = df['pickup_latitude'][0],df['pickup_longitude'][0],df['dropoff_latitude'][0],df['dropoff_longitude'][0]\n",
    "request_str = 'http://localhost:8989/route?point=' + str(a) + '%2C' + str(b) + '&point=' + str(\n",
    "            c) + '%2C' + str(d) + '&vehicle=car'\n",
    "request = Request(request_str)\n",
    "res=requests.get(request_str)\n",
    "print(\"Distance = {}\".format(json.loads(res.text)['paths'][0]['distance']))\n",
    "print(\"Time = {}\".format(json.loads(res.text)['paths'][0]['time']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To calculate pool_window.\n",
    "## The dataframe will be divided into pools.We run algorithm for each of these pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ceil_dt(dt, delta):\n",
    "    return datetime.min + math.ceil((dt - datetime.min) / delta) * delta\n",
    "\n",
    "pool_time_window = 10 # Change pool time window\n",
    "df['pool_window'] = df['pickup_time'].apply(lambda x: ceil_dt(parser.parse(x), timedelta(minutes=pool_time_window)))\n",
    "df['delay'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is expected to be calculated and stored in a csv file. Later loaded into a dataframe before running the algorithm.The columns that should be stored in the csv files are same as following dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distance =  pd.DataFrame(columns = ['pickup_h3','dropoff_h3','distance','duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The nodes of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,idx,data):\n",
    "        self.id = idx\n",
    "        self.pickup_location = (data.pickup_latitude,data.pickup_longitude,data.pickup_h3)\n",
    "        self.dropoff_location = (data.dropoff_latitude,data.dropoff_longitude,data.dropoff_h3)\n",
    "        self.pickup_time = data.pickup_time\n",
    "        self.dropoff_time = data.dropoff_time\n",
    "        self.distance = df_distance[(df_distance.pickup_h3==data.pickup_h3)&(df_distance.dropoff_h3==data.dropoff_h3)].distance\n",
    "        self.duration = df_distance[(df_distance.pickup_h3==data.pickup_h3)&(df_distance.dropoff_h3==data.dropoff_h3)].duration\n",
    "        self.delay = data.delay\n",
    "        #TBD.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_pairs(node_a,node_b,trip_type):\n",
    "    if trip_type == 1:\n",
    "        #Combination LGA--> a -->b\n",
    "        #if no distance call graphhopper \n",
    "        LGA_a_dist = df_distance.loc[(node_a.pickup_location[2],node_a.dropoff_location[2])]['distance']\n",
    "        a_b_dist = df_distance.loc[(node_a.dropoff_location[2],node_b.dropoff_location[2])]['distance']\n",
    "        LGA_a_dur = df_distance.loc[(node_a.pickup_location[2],node_a.dropoff_location[2])]['duration']\n",
    "        a_b_dur = df_distance.loc[(node_a.dropoff_location[2],node_b.dropoff_location[2])]['duration']\n",
    "        \n",
    "        #Combination LGA--> b -->a\n",
    "        LGA_b_dist = df_distance.loc[(node_b.pickup_location[2],node_b.dropoff_location[2])]['distance']\n",
    "        b_a_dist = df_distance.loc[(node_b.dropoff_location[2],node_a.dropoff_location[2])]['distance']\n",
    "        LGA_b_dur = df_distance.loc[(node_b.pickup_location[2],node_b.dropoff_location[2])]['duration']\n",
    "        b_a_dur = df_distance.loc[(node_b.dropoff_location[2],node_a.dropoff_location[2])]['duration']\n",
    "        \n",
    "        path_1_total_dis,path_1_total_dur = LGA_a_dist + a_b_dist,LGA_a_dur + a_b_dur \n",
    "        path_1_a_dur,path_1_b_dur = LGA_a_dur,path_1_total_dur\n",
    "        \n",
    "        path_2_total_dis,path_2_total_dur = LGA_b_dist+b_a_dist,LGA_b_dur+b_a_dur\n",
    "        path_2_a_dur,path_2_b_dur         = path_2_total_dur ,LGA_b_dur\n",
    "               \n",
    "    else:\n",
    "        #Combination a--> b --> LGA\n",
    "        a_b_dist = df_distance.loc[(node_a.pickup_location[2],node_b.pickup_location[2])]['distance']\n",
    "        b_LGA_dist = df_distance.loc[(node_b.pickup_location[2],node_b.dropoff_location[2])]['distance'] \n",
    "        a_b_dur = df_distance.loc[(node_a.pickup_location[2],node_b.pickup_location[2])]['duration']\n",
    "        b_LGA_dur = df_distance.loc[(node_b.pickup_location[2],node_b.dropoff_location[2])]['duration']\n",
    "        \n",
    "        #Combination b--> a --> LGA\n",
    "        b_a_dist = df_distance.loc[(node_b.pickup_location[2],node_a.pickup_location[2])]['distance']\n",
    "        a_LGA_dist = df_distance.loc[(node_a.pickup_location[2],node_a.dropoff_location[2])]['distance'] \n",
    "        b_a_dur = df_distance.loc[(node_b.pickup_location[2],node_a.pickup_location[2])]['duration']\n",
    "        a_LGA_dur = df_distance.loc[(node_a.pickup_location[2],node_a.dropoff_location[2])]['duration']\n",
    "        \n",
    "        path_1_total_dis,path_1_total_dur = a_b_dist + b_LGA_dist,a_b_dur + b_LGA_dur \n",
    "        path_1_a_dur,path_1_b_dur = path_1_total_dur,b_LGA_dur\n",
    "        \n",
    "        path_2_total_dis,path_2_total_dur, = b_a_dist+a_LGA_dist,b_a_dur+a_LGA_dur\n",
    "        path_2_a_dur,path_2_b_dur         = a_LGA_dur,path_2_total_dur\n",
    "        \n",
    "    return ((path_1_total_dis,path_1_total_dur,path_1_a_dur,path_1_b_dur),( path_2_total_dis,path_2_total_dur,path_2_a_dur,path_2_b_dur))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_edge_weight(node_a,node_b):\n",
    "    path1,path2 = get_all_pairs(node_a,node_b,1)\n",
    "    minimum_distance = float('inf')\n",
    "    for path in (path1,path2):\n",
    "        distance_contraint = (path[0] <= a.distance + b.distance)\n",
    "        delay_constraint = (path[2] <= a.duration + a.delay) & (path[3] <= b.duration + b.delay)\n",
    "        #add social constraint too...\n",
    "        \n",
    "        \n",
    "        if distance_contraint and delay_constraint and path[0]< minimum_distance:\n",
    "            minimum_distance = path[0]\n",
    "    distance_saved = a.distance + b.distance - minimum_distance\n",
    "    return distance_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rsg(G):\n",
    "    for node_a,node_b in list(combinations(G,2)):\n",
    "        distance_saved = calculate_edge_weight(node_a,node_b)\n",
    "        if distance_saved!= float('inf'):\n",
    "            G.add_edge(node_a,node_b, weight={'distance_saved': distance_saved})\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The algorithm begins from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trips in (df.groupby['pool_window']):\n",
    "    nodes = []\n",
    "    trips = trips.reset_index()\n",
    "    for idx, row in df.iterrows():\n",
    "        nodes.append(Node(idx,df.iloc[idx]))\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    graphs.append(G)\n",
    "    \n",
    "#Start of the code\n",
    "weight_matches = []\n",
    "for g in graphs:\n",
    "    ride_sharing_graph = get_rsg(g)\n",
    "    #maximum weighted algorithm\n",
    "    match = nx.max_weight_matching(g, weight='final', maxcardinality=True)\n",
    "    g_match = nx.Graph()\n",
    "    for ii in match:\n",
    "        g_match.add_edge(ii[0],ii[1])\n",
    "    weight_matches.append(g_match)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
